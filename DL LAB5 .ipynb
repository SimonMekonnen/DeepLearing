{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjwqY3v5D/1EXEuLoS1nY5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SimonMekonnen/DeepLearing/blob/main/DL%20LAB5%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   Simon Mekonnen\n",
        "*   UGR/9508/12\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-yxEXj_OTWYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "i-EQYihsTuz9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer():\n",
        "    def __init__(self, input_features, output_neurons):\n",
        "        # Initialize weights with small random values and biases with zeros\n",
        "        self.weights = 0.01 * torch.rand(output_neurons, input_features)\n",
        "        self.biases = torch.zeros(1, output_neurons)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        # Compute the weighted sum and add biases for the forward pass\n",
        "        self.input_data = input_data\n",
        "        self.output = torch.matmul(input_data, self.weights.T) + self.biases\n",
        "\n",
        "    def backward(self, gradient_values):\n",
        "        # Calculate gradients for weights, biases, and input values during backward pass\n",
        "        self.dweights = torch.matmul(self.input_data.T, gradient_values)\n",
        "        self.dbiases = torch.sum(gradient_values, axis=0, keepdims=True)\n",
        "        self.dinputs = torch.matmul(gradient_values, self.weights.T)"
      ],
      "metadata": {
        "id": "ZgI6HUkzTgYn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLUActivation:\n",
        "\n",
        "    def forward(self, input_values):\n",
        "        # Apply ReLU activation: if input_values < 0, set to 0, else keep the value\n",
        "        output_values = torch.max(input_values, torch.tensor(0.0))\n",
        "        return output_values\n",
        "\n",
        "    def backward(self, gradient_values):\n",
        "        # Clone the gradient values to avoid in-place modification\n",
        "        self.dinputs = gradient_values.clone()\n",
        "\n",
        "        # Derivative of ReLU activation on the inputs\n",
        "        self.dinputs[self.input_values <= 0] = 0\n",
        "        # Note: Assuming self.input_values is the input data during the forward pass"
      ],
      "metadata": {
        "id": "jEFetrVBUPzJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxActivation:\n",
        "    def forward(self, input_values):\n",
        "        # Calculate exponentials of input values\n",
        "        exp_values = torch.exp(input_values)\n",
        "        # Get the shape of the input\n",
        "        input_shape = input_values.shape\n",
        "        # Calculate the sum of exponentials along the second axis\n",
        "        sum_exp_values = torch.sum(exp_values, axis=1, keepdims=True)\n",
        "        # Compute the softmax result by dividing each element by the sum\n",
        "        result = exp_values / sum_exp_values\n",
        "        self.output = result\n",
        "        return result\n",
        "\n",
        "    def backward(self, gradient_values):\n",
        "        # Initialize an empty tensor with the same shape as gradient_values\n",
        "        self.dinputs = torch.empty_like(gradient_values)\n",
        "\n",
        "        for index, (single_output, single_gradient) in enumerate(zip(self.output, gradient_values)):\n",
        "            # Reshape the single_output to a column vector\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "            # Calculate the Jacobian matrix for the softmax\n",
        "            jacobian_matrix = torch.diagflat(single_output) - torch.matmul(single_output, single_output.T)\n",
        "            # Multiply the Jacobian matrix by the gradient to get the derivative\n",
        "            self.dinputs[index] = torch.matmul(jacobian_matrix, single_gradient)\n"
      ],
      "metadata": {
        "id": "06JS4boOU13I"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relu = ReLUActivation()\n",
        "\n",
        "softmax = SoftmaxActivation()"
      ],
      "metadata": {
        "id": "hb5244VAU4F5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manual_seed = 42\n",
        "torch.manual_seed(manual_seed)\n",
        "features = 2\n",
        "neuron_1 = 2\n",
        "output_class = 2\n",
        "samples = 10\n",
        "lower_bound = 0\n",
        "upper_bound = 10000\n",
        "input_layer = (upper_bound - lower_bound) * torch.rand(samples, features) + lower_bound\n",
        "layer_1 = DenseLayer(features, neuron_1)\n",
        "layer_1.forward(input_layer)\n",
        "output_1 = relu.forward(layer_1.output)\n",
        "output_layer = DenseLayer(output_1.shape[1], output_class)\n",
        "output_layer.forward(output_1)\n",
        "final_output_1 = softmax.forward(output_layer.output)\n",
        "print(final_output_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiWfvmwdcJpc",
        "outputId": "bae4edd0-31b1-408c-cd86-d1d7251cf4ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4773, 0.5227],\n",
            "        [0.4826, 0.5174],\n",
            "        [0.4873, 0.5127],\n",
            "        [0.4863, 0.5137],\n",
            "        [0.4870, 0.5130],\n",
            "        [0.4810, 0.5190],\n",
            "        [0.4821, 0.5179],\n",
            "        [0.4854, 0.5146],\n",
            "        [0.4818, 0.5182],\n",
            "        [0.4884, 0.5116]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss:\n",
        "  def calculate(self, predicted_values, ground_truth):\n",
        "    sample_losses = self.forward(predicted_values, ground_truth)\n",
        "    data_loss = torch.mean(sample_losses)\n",
        "    return data_loss\n"
      ],
      "metadata": {
        "id": "plA6O5t0W5tF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CategoricalCrossEntropyLoss(Loss):\n",
        "  def forward(self, predictions, true_labels):\n",
        "    num_samples = len(predictions)\n",
        "\n",
        "    predictions_clipped = torch.clip(predictions, 1e-8, 1 - 1e-8)\n",
        "\n",
        "    if len(true_labels.shape) == 1:\n",
        "      correct_confidences = predictions_clipped[\n",
        "      range(num_samples),\n",
        "      true_labels\n",
        "      ]\n",
        "    elif len(true_labels.shape) == 2:\n",
        "      correct_confidences = torch.sum(\n",
        "      predictions_clipped * true_labels,\n",
        "      axis=1\n",
        "      )\n",
        "    negative_log_likelihoods = -torch.log(correct_confidences)\n",
        "    return negative_log_likelihoods\n",
        "\n",
        "  def backward(self, gradients, true_labels):\n",
        "    num_samples = len(gradients)\n",
        "    num_labels = len(gradients[0])\n",
        "    if len(true_labels.shape) == 1:\n",
        "      true_labels = torch.eye(num_labels)[true_labels]\n",
        "    self.deltas = -true_labels / gradients\n",
        "    self.deltas = self.deltas / num_samples\n"
      ],
      "metadata": {
        "id": "tH41RQS-W7Z6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxCategoricalCrossEntropy():\n",
        "  def __init__(self):\n",
        "    self.softmax_activation = SoftmaxActivation()\n",
        "    self.categorical_crossentropy_loss = CategoricalCrossEntropyLoss()\n",
        "\n",
        "  def forward(self, inputs, true_labels):\n",
        "    self.softmax_activation.forward(inputs)\n",
        "    self.output = self.softmax_activation.output\n",
        "    return self.categorical_crossentropy_loss.calculate(self.output, true_labels)\n",
        "\n",
        "  def backward(self, gradients, true_labels):\n",
        "    num_samples = len(gradients)\n",
        "    if len(true_labels.shape) == 2:\n",
        "      true_labels = torch.argmax(true_labels, axis=1)\n",
        "\n",
        "    self.deltas = gradients.clone()\n",
        "    self.deltas[range(num_samples), true_labels] -= 1\n",
        "    self.deltas = self.deltas / num_samples\n"
      ],
      "metadata": {
        "id": "MO840VQ7Xdw8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation_loss = SoftmaxCategoricalCrossEntropy()\n"
      ],
      "metadata": {
        "id": "DmblvdPCrGaU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = torch.randint(output_class, (samples,))\n",
        "loss = activation_loss.forward(final_output_1,target)\n"
      ],
      "metadata": {
        "id": "Ost-jOOBrbUW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation_loss.backward(activation_loss.output, target)\n",
        "layer_1.backward(activation_loss.deltas)\n",
        "print(layer_1.dweights)\n",
        "print(layer_1.biases)\n"
      ],
      "metadata": {
        "id": "1Q5WLDtuGP4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec2deb31-dc89-4082-c898-1298146bacb4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  752.4683,  -752.4681],\n",
            "        [ 1116.2562, -1116.2561]])\n",
            "tensor([[0., 0.]])\n"
          ]
        }
      ]
    }
  ]
}